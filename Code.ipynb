{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "0. Imports + Dataset"
      ],
      "metadata": {
        "id": "is1LuiVTOD65"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O1fVsWOz5pXK",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ecb2d15a-64e5-45b4-9582-cd31adb72972"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting mediapipe\n",
            "  Downloading mediapipe-0.8.10.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (32.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 32.9 MB 1.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from mediapipe) (1.21.6)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from mediapipe) (1.2.0)\n",
            "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.7/dist-packages (from mediapipe) (22.1.0)\n",
            "Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.7/dist-packages (from mediapipe) (4.6.0.66)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from mediapipe) (3.2.2)\n",
            "Requirement already satisfied: protobuf<4,>=3.11 in /usr/local/lib/python3.7/dist-packages (from mediapipe) (3.17.3)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf<4,>=3.11->mediapipe) (1.15.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mediapipe) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mediapipe) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mediapipe) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mediapipe) (1.4.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib->mediapipe) (4.1.1)\n",
            "Installing collected packages: mediapipe\n",
            "Successfully installed mediapipe-0.8.10.1\n"
          ]
        }
      ],
      "source": [
        "!pip install mediapipe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GqRsPYZb59g0"
      },
      "outputs": [],
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "import sklearn\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn import preprocessing\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import *\n",
        "from numpy.linalg import norm\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "import mediapipe as mp\n",
        "import cv2\n",
        "import os\n",
        "from xgboost import XGBClassifier\n",
        "from tqdm import tqdm\n",
        "\n",
        "import pickle\n",
        "import copy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! gdown --id 18WNBVg98h_JKXCSKaZDELalBXZkvExI3"
      ],
      "metadata": {
        "id": "j0PNrqfiO_he",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c4b4424-beba-4573-b348-80f0a211d852"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gdown/cli.py:131: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  category=FutureWarning,\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=18WNBVg98h_JKXCSKaZDELalBXZkvExI3\n",
            "To: /content/trainSet_yoga82.csv\n",
            "100% 45.7M/45.7M [00:00<00:00, 268MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Helper functions"
      ],
      "metadata": {
        "id": "sVIHQIFvOKHa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mHB9ucoB5pXM",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "def run_models(x_train, x_test, y_train, y_test, custom):\n",
        "    \n",
        "    model_dict = {}\n",
        "    \n",
        "    ## logistic regression\n",
        "    model_logistic = LogisticRegression()\n",
        "    model_logistic.fit(x_train,y_train)\n",
        "    y_pred = model_logistic.predict(x_test)\n",
        "    print(25*\"##\")\n",
        "    print(\"LOGISTIC REGRESSION\")\n",
        "    print(\"Testing Data:\")\n",
        "    print(\"Accuracy:\",accuracy_score(y_test,y_pred))\n",
        "    print(\"F1 Score:\",f1_score(y_test,y_pred, average=\"weighted\"))\n",
        "    #print(classification_report(y_test,y_pred))\n",
        "    #print(25*\"**\")\n",
        "    y_pred = model_logistic.predict(x_train)\n",
        "    print(25*\"--\")\n",
        "    print(\"Training Data:\")\n",
        "    print(\"Accuracy:\",accuracy_score(y_train,y_pred))\n",
        "    print(\"F1 Score:\",f1_score(y_train,y_pred, average=\"weighted\"))\n",
        "    ##print(classification_report(y_train,y_pred))\n",
        "    \n",
        "    ## SGD classifier\n",
        "    model_SGD = SGDClassifier()\n",
        "    model_SGD.fit(x_train,y_train)\n",
        "    y_pred = model_SGD.predict(x_test)\n",
        "    print(25*\"##\")\n",
        "    print(\"SGD CLASSIFIER\")\n",
        "    print(\"Testing Data:\")\n",
        "    print(\"Accuracy:\",accuracy_score(y_test,y_pred))\n",
        "    print(\"F1 Score:\",f1_score(y_test,y_pred, average=\"weighted\"))\n",
        "    #print(classification_report(y_test,y_pred))\n",
        "    #print(25*\"**\")\n",
        "    y_pred = model_SGD.predict(x_train)\n",
        "    print(25*\"--\")\n",
        "    print(\"Training Data:\")\n",
        "    print(\"Accuracy:\",accuracy_score(y_train,y_pred))\n",
        "    print(\"F1 Score:\",f1_score(y_train,y_pred, average=\"weighted\"))\n",
        "    ##print(classification_report(y_train,y_pred))\n",
        "    \n",
        "    ## Naive Bayes\n",
        "    model_NB = GaussianNB()\n",
        "    model_NB.fit(x_train,y_train)\n",
        "    y_pred = model_NB.predict(x_test)\n",
        "    print(25*\"##\")\n",
        "    print(\"GAUSSIAN NAIVE BAYES\")\n",
        "    print(\"Testing Data:\")\n",
        "    print(\"Accuracy:\",accuracy_score(y_test,y_pred))\n",
        "    print(\"F1 Score:\",f1_score(y_test,y_pred, average=\"weighted\"))\n",
        "    #print(classification_report(y_test,y_pred))\n",
        "    #print(25*\"**\")\n",
        "    y_pred = model_NB.predict(x_train)\n",
        "    print(25*\"--\")\n",
        "    print(\"Training Data:\")\n",
        "    print(\"Accuracy:\",accuracy_score(y_train,y_pred))\n",
        "    print(\"F1 Score:\",f1_score(y_train,y_pred, average=\"weighted\"))\n",
        "    ##print(classification_report(y_train,y_pred))\n",
        "    \n",
        "    ## Random Forest\n",
        "    model_RF = RandomForestClassifier()\n",
        "    model_RF.fit(x_train,y_train)\n",
        "    y_pred = model_RF.predict(x_test)\n",
        "    print(25*\"##\")\n",
        "    print(\"RANDOM FOREST\")\n",
        "    print(\"Testing Data:\")\n",
        "    print(\"Accuracy:\",accuracy_score(y_test,y_pred))\n",
        "    print(\"F1 Score:\",f1_score(y_test,y_pred, average=\"weighted\"))\n",
        "    #print(classification_report(y_test,y_pred))\n",
        "    #print(25*\"**\")\n",
        "    y_pred = model_RF.predict(x_train)\n",
        "    print(25*\"--\")\n",
        "    print(\"Training Data:\")\n",
        "    print(\"Accuracy:\",accuracy_score(y_train,y_pred))\n",
        "    print(\"F1 Score:\",f1_score(y_train,y_pred, average=\"weighted\"))\n",
        "    ##print(classification_report(y_train,y_pred))\n",
        "    \n",
        "    ## XGBoost\n",
        "    model_xgboost = XGBClassifier()\n",
        "    model_xgboost.fit(x_train,y_train)\n",
        "    y_pred = model_xgboost.predict(x_test)\n",
        "    print(25*\"##\")\n",
        "    print(\"XGBOOST\")\n",
        "    print(\"Testing Data:\")\n",
        "    print(\"Accuracy:\",accuracy_score(y_test,y_pred))\n",
        "    print(\"F1 Score:\",f1_score(y_test,y_pred, average=\"weighted\"))\n",
        "    #print(classification_report(y_test,y_pred))\n",
        "    #print(25*\"**\")\n",
        "    y_pred = model_xgboost.predict(x_train)\n",
        "    print(25*\"--\")\n",
        "    print(\"Training Data:\")\n",
        "    print(\"Accuracy:\",accuracy_score(y_train,y_pred))\n",
        "    print(\"F1 Score:\",f1_score(y_train,y_pred, average=\"weighted\"))\n",
        "    ##print(classification_report(y_train,y_pred))\n",
        "    \n",
        "    ## AdaBoost\n",
        "    model_adaboost = AdaBoostClassifier()\n",
        "    model_adaboost.fit(x_train,y_train)\n",
        "    y_pred = model_adaboost.predict(x_test)\n",
        "    print(25*\"##\")\n",
        "    print(\"ADABOOST\")\n",
        "    print(\"Testing Data:\")\n",
        "    print(\"Accuracy:\",accuracy_score(y_test,y_pred))\n",
        "    print(\"F1 Score:\",f1_score(y_test,y_pred, average=\"weighted\"))\n",
        "    #print(classification_report(y_test,y_pred))\n",
        "    #print(25*\"**\")\n",
        "    y_pred = model_adaboost.predict(x_train)\n",
        "    print(25*\"--\")\n",
        "    print(\"Training Data:\")\n",
        "    print(\"Accuracy:\",accuracy_score(y_train,y_pred))\n",
        "    print(\"F1 Score:\",f1_score(y_train,y_pred, average=\"weighted\"))\n",
        "    ##print(classification_report(y_train,y_pred))\n",
        "    \n",
        "    ## KNN\n",
        "    model_knn = KNeighborsClassifier()\n",
        "    model_knn.fit(x_train,y_train)\n",
        "    y_pred = model_knn.predict(x_test)\n",
        "    print(25*\"##\")\n",
        "    print(\"KNN CLASSIFIER\")\n",
        "    print(\"Testing Data:\")\n",
        "    print(\"Accuracy:\",accuracy_score(y_test,y_pred))\n",
        "    print(\"F1 Score:\",f1_score(y_test,y_pred, average=\"weighted\"))\n",
        "    #print(classification_report(y_test,y_pred))\n",
        "    #print(25*\"**\")\n",
        "    y_pred = model_knn.predict(x_train)\n",
        "    print(25*\"--\")\n",
        "    print(\"Training Data:\")\n",
        "    print(\"Accuracy:\",accuracy_score(y_train,y_pred))\n",
        "    print(\"F1 Score:\",f1_score(y_train,y_pred, average=\"weighted\"))\n",
        "    ##print(classification_report(y_train,y_pred))\n",
        "    \n",
        "    ## SVM\n",
        "    model_SVM = SVC()\n",
        "    model_SVM.fit(x_train,y_train)\n",
        "    y_pred = model_SVM.predict(x_test)\n",
        "    print(25*\"##\")\n",
        "    print(\"SVM\")\n",
        "    print(\"Testing Data:\")\n",
        "    print(\"Accuracy:\",accuracy_score(y_test,y_pred))\n",
        "    print(\"F1 Score:\",f1_score(y_test,y_pred, average=\"weighted\"))\n",
        "    #print(classification_report(y_test,y_pred))\n",
        "    #print(25*\"**\")\n",
        "    y_pred = model_SVM.predict(x_train)\n",
        "    print(25*\"--\")\n",
        "    print(\"Training Data:\")\n",
        "    print(\"Accuracy:\",accuracy_score(y_train,y_pred))\n",
        "    print(\"F1 Score:\",f1_score(y_train,y_pred, average=\"weighted\"))\n",
        "    ##print(classification_report(y_train,y_pred))\n",
        "\n",
        "    model_dict[\"logistic\"] = model_logistic\n",
        "    model_dict[\"SGD\"] = model_SGD\n",
        "    model_dict[\"RF\"] = model_RF\n",
        "    model_dict[\"KNN\"] = model_knn\n",
        "    model_dict[\"SVM\"] = model_SVM\n",
        "    model_dict[\"XGB\"] = model_xgboost\n",
        "    model_dict[\"ADA\"] = model_adaboost\n",
        "    model_dict[\"NB\"] = model_NB\n",
        "    \n",
        "    name = \"raw_feature_model_dict\"\n",
        "    if custom:\n",
        "        name = \"custom_feature_model_dict.pkl\"\n",
        "    \n",
        "    with open(name,\"wb\") as handle:\n",
        "        pickle.dump(model_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "        \n",
        "    return model_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jzK0_XLwAhUi"
      },
      "outputs": [],
      "source": [
        "#SCORING\n",
        "def get_cosine_similarity_results(x_train, x_test, y_train, y_test, features):\n",
        "    #print(x_train[:10])\n",
        "    #print(y_train[:10])\n",
        "    y_pred = []\n",
        "    \n",
        "    dataset = pd.concat([x_train, y_train], axis=1)\n",
        "    dict1 = {} #pose -> 132 size array with mean keypoints\n",
        "\n",
        "    for pose in range(0,81):\n",
        "      temp = dataset.loc[dataset['label'] == pose].mean()\n",
        "      temp = temp.drop(labels=['label'])\n",
        "      dict1[pose] = temp\n",
        "    #print(dict1)\n",
        "    \n",
        "    with open(\"cosine_dict.pkl\",\"wb\") as handle:\n",
        "        pickle.dump(dict1,handle)\n",
        "        \n",
        "    feature_wise_cosine_similarity = {} ## i'th data_point -> {}\n",
        "    \n",
        "    feature_wise_cosine_similarity[\"data_point\"] = []\n",
        "    feature_wise_cosine_similarity[\"actual_pose\"] = []\n",
        "    feature_wise_cosine_similarity[\"predicted_pose\"] = []\n",
        "    \n",
        "    for i in tqdm(range(x_test.shape[0])):\n",
        "\n",
        "        curr_row = [x_test.iloc[i]]\n",
        "\n",
        "        curr_y = -1\n",
        "        curr_sim_score = -float(\"inf\")\n",
        "\n",
        "        for pose in range(0,81):\n",
        "          row = dict1[pose]\n",
        "          #print(row)\n",
        "          #print(\"----\")\n",
        "          #print(curr_row)\n",
        "          row = row.to_numpy()\n",
        "          curr_row = np.array(curr_row)\n",
        "          sim_score = cosine_similarity(row.reshape(1, -1),curr_row.reshape(1,-1))\n",
        "\n",
        "          if sim_score > curr_sim_score:\n",
        "            curr_sim_score = sim_score\n",
        "            curr_y = pose\n",
        "\n",
        "        \"\"\"for j in range(x_train.shape[0]):\n",
        "\n",
        "            row = [x_train.iloc[j,:]]\n",
        "\n",
        "            sim_score = cosine_similarity(row,curr_row)\n",
        "\n",
        "            if sim_score > curr_sim_score:\n",
        "                curr_sim_score = sim_score\n",
        "                curr_y = y_train.iloc[j]\"\"\"\n",
        "        \n",
        "        feature_wise_cosine_similarity[\"data_point\"].append(i)\n",
        "        feature_wise_cosine_similarity[\"predicted_pose\"].append(le.inverse_transform([curr_y])[0])\n",
        "        feature_wise_cosine_similarity[\"actual_pose\"].append(le.inverse_transform([list(y_test)[i]])[0])\n",
        "        \n",
        "        row = dict1[curr_y]\n",
        "        for j in range(len(features)-1):\n",
        "            \n",
        "            if features[j] not in feature_wise_cosine_similarity:\n",
        "                feature_wise_cosine_similarity[features[j]] = []\n",
        "                \n",
        "            n1 = row[j]\n",
        "            n2 = curr_row[0][j]\n",
        "            sim = 1 - abs(n1 - n2) / (n1 + n2)\n",
        "            feature_wise_cosine_similarity[features[j]] = sim\n",
        "        \n",
        "        y_pred.append(curr_y)\n",
        "\n",
        "    y_test_list = list(y_test)\n",
        "    \n",
        "    print(25*\"##\")\n",
        "    print(\"COSINE SIMILARITY RESULTS:\")\n",
        "    print(\"Accuracy:\",accuracy_score(y_test_list,y_pred))\n",
        "    print(\"F1 Score:\",f1_score(y_test_list,y_pred, average=\"weighted\"))\n",
        "    \n",
        "    correction_df = pd.DataFrame(feature_wise_cosine_similarity)\n",
        "    \n",
        "    if len(features) >= 100:\n",
        "        correction_df.to_csv(\"correction_dataframe_raw.csv\")\n",
        "    else:\n",
        "        correction_df.to_csv(\"correction_dataframe_custom.csv\")\n",
        "    \n",
        "    print(25*\"##\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def cosine_eval(x_test, y_test, y_pred, features):\n",
        "    #print(x_train[:10])\n",
        "    #print(y_train[:10])\n",
        "    \n",
        "    with open(\"cosine_dict.pkl\",\"rb\") as handle:\n",
        "        dict1 = pickle.load(handle)\n",
        "        \n",
        "    feature_wise_cosine_similarity = {} ## i'th data_point -> {}\n",
        "    \n",
        "    feature_wise_cosine_similarity[\"data_point\"] = []\n",
        "    feature_wise_cosine_similarity[\"actual_pose\"] = []\n",
        "    feature_wise_cosine_similarity[\"predicted_pose\"] = []\n",
        "    \n",
        "    for i in tqdm(range(x_test.shape[0])):\n",
        "\n",
        "        curr_row = [x_test.iloc[i]]\n",
        "      \n",
        "        feature_wise_cosine_similarity[\"data_point\"].append(i)\n",
        "        \n",
        "        feature_wise_cosine_similarity[\"predicted_pose\"].append(le.inverse_transform([y_pred[i]])[0])\n",
        "        feature_wise_cosine_similarity[\"actual_pose\"].append(le.inverse_transform([list(y_test)[i]])[0])\n",
        "        \n",
        "        row = dict1[y_pred[i]]\n",
        "        for j in range(len(features)-1):\n",
        "            \n",
        "            if features[j] not in feature_wise_cosine_similarity:\n",
        "                feature_wise_cosine_similarity[features[j]] = []\n",
        "                \n",
        "            n1 = row[j]\n",
        "            n2 = curr_row[0][j]\n",
        "            sim = 1 - abs(n1 - n2) / (n1 + n2)\n",
        "            feature_wise_cosine_similarity[features[j]] = sim\n",
        "        \n",
        "\n",
        "    y_test_list = list(y_test)\n",
        "    \n",
        "    print(\"dataframe saved with eval results!!\")\n",
        "    \n",
        "    correction_df = pd.DataFrame(feature_wise_cosine_similarity)\n",
        "    featurecopy = copy.deepcopy(feature_wise_cosine_similarity)\n",
        "    del featurecopy['data_point']\n",
        "    del featurecopy['actual_pose']\n",
        "    del featurecopy['predicted_pose']\n",
        "    featurecopy = sorted(featurecopy.items(), key=lambda x: x[1])\n",
        "    #print(featurecopy[0][0])\n",
        "    # dealing with predicted_pose key\n",
        "    # print : top 3 keypoints with least similarity score\n",
        "    print(25*\"##\")\n",
        "    print(\"Top 3 keypoints to be corrected : \")\n",
        "    print()\n",
        "    count = 0\n",
        "    for k in range(0, len(featurecopy)):\n",
        "        \n",
        "        if count>2:\n",
        "            break\n",
        "    \n",
        "        if \"centroid\" not in featurecopy[k][0]:\n",
        "            print(featurecopy[k][0], \":\", featurecopy[k][1])\n",
        "            count += 1\n",
        "    print(25*'##')\n",
        "    \n",
        "    if len(features) >= 100:\n",
        "        correction_df.to_csv(\"correction_dataframe_raw.csv\")\n",
        "    else:\n",
        "        correction_df.to_csv(\"correction_dataframe_custom.csv\") "
      ],
      "metadata": {
        "id": "E6L2DVM1Omaw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r8yDasmZ5pXO",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "#NOT NEEDED\n",
        "def get_raw_data_results(df):\n",
        "    \n",
        "    ## converting output to numeric values\n",
        "    le.fit(df[\"Pose\"])\n",
        "    df[\"label\"] = le.transform(df[\"Pose\"])\n",
        "    \n",
        "    ## dropping Pose column\n",
        "    df.drop(columns=[\"Pose\",\"ImgNum\"],inplace=True)\n",
        "    \n",
        "    ## feature designing for train and test\n",
        "    df #= get_designed_data_df(df)\n",
        "    \n",
        "    x = df.iloc[:,:-1]\n",
        "    y = df.iloc[:,-1]\n",
        "    \n",
        "    x_train, x_test, y_train, y_test = train_test_split(x,y,stratify=y,test_size=0.25, random_state=0)\n",
        "\n",
        "    \"\"\"dataset = pd.concat([x_train, y_train], axis=1)\n",
        "    dict1 = {} #pose -> 132 size array with mean keypoints\n",
        "\n",
        "    for pose in range(0,81):\n",
        "      temp = dataset.loc[dataset['label'] == pose].mean()\n",
        "      dict1[pose] = temp\n",
        "    print(dict1)\"\"\"\n",
        "\n",
        "    \n",
        "    ## running different models on this\n",
        "    model_dict = run_models(x_train, x_test, y_train, y_test,False)\n",
        "    \n",
        "    return model_dict\n",
        "    ## get cosine similarity results\n",
        "    #get_cosine_similarity_results(x_train, x_test, y_train, y_test, df.columns)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_angle(p1, p2, p3):\n",
        "    \n",
        "    \"\"\"\n",
        "        returns angle between p1,p2,p3 with p2 as the pivot\n",
        "    \"\"\"\n",
        "    \n",
        "    v1 = p1-p2\n",
        "    v2 = p3-p2\n",
        "    \n",
        "    num = np.inner(v1,v2)\n",
        "    den = (np.sqrt((v1**2).sum())) * (np.sqrt((v2**2).sum()))\n",
        "    \n",
        "    return np.degrees(np.arccos(num/den))"
      ],
      "metadata": {
        "id": "35yBGSA1OsFH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_designed_data_df(df):\n",
        "    \n",
        "    d = {}\n",
        "\n",
        "    for i in tqdm(range(df.shape[0])):\n",
        "\n",
        "        j=0\n",
        "        while j<df.shape[1]-1:\n",
        "\n",
        "            col_name = df.columns[j].split(\"-\")[0]\n",
        "\n",
        "            data_point = np.array([df.iloc[i,j], df.iloc[i,j+1], df.iloc[i,j+2]])\n",
        "            col_name = col_name.lower()\n",
        "\n",
        "            if col_name not in d:\n",
        "                d[col_name] = []\n",
        "\n",
        "            d[col_name].append(data_point)\n",
        "            j+=4\n",
        "\n",
        "        if \"label\" not in d:\n",
        "            d[\"label\"] = []\n",
        "        d[\"label\"].append(df.iloc[i,j]) \n",
        "        \n",
        "    return get_custom_features(pd.DataFrame(d))"
      ],
      "metadata": {
        "id": "hGRTrHXTOun5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_plane(p1, p2, p3):\n",
        "    #equation of plane : ax + by + cz + d = 0\n",
        "    a1 = p2[0] - p1[0]\n",
        "    b1 = p2[1] - p1[1]\n",
        "    c1 = p2[2] - p1[2]\n",
        "    a2 = p3[0] - p1[0]\n",
        "    b2 = p3[1] - p1[1]\n",
        "    c2 = p3[2] - p1[2]\n",
        "    a = b1 * c2 - b2 * c1\n",
        "    b = a2 * c1 - a1 * c2\n",
        "    c = a1 * b2 - b1 * a2\n",
        "    d = (- a * p1[0] - b * p1[1] - c * p1[2])\n",
        "\n",
        "    return [a,b,c,d]"
      ],
      "metadata": {
        "id": "PLwefCFPMPwG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jh55FFVF5pXO",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "def get_custom_features(df):\n",
        "    \n",
        "    ## add features\n",
        "\n",
        "    d = {}\n",
        "\n",
        "    ## angles\n",
        "    d[\"max_hand_angle\"] = []\n",
        "    d[\"min_hand_angle\"] = []\n",
        "    d[\"max_foot_angle\"] = []\n",
        "    d[\"min_foot_angle\"] = []\n",
        "\n",
        "    d[\"max_elbow_to_knee_angle\"] = []\n",
        "    d[\"min_elbow_to_knee_angle\"] = []\n",
        "\n",
        "    d[\"knee_to_knee_angle\"] = []\n",
        "    d[\"elbow_to_elbow_angle\"] = []\n",
        "\n",
        "    #d[\"heel_at_nose_angle\"] = []\n",
        "    #d[\"hand_at_nose_angle\"] = []\n",
        "    d[\"max_nose_to_heel_angle\"] = []\n",
        "\n",
        "    ## distances\n",
        "    d[\"feet_to_shoulder_ratio\"] = []\n",
        "    d[\"hand_to_shoulder_ratio\"] = []\n",
        "\n",
        "    # symmetry\n",
        "    \"\"\"d[\"wrist_sagittal\"] = []\n",
        "    d[\"elbow_sagittal\"] = []\n",
        "    d[\"knee_sagittal\"] = []\n",
        "    d[\"ankle_sagittal\"] = []\"\"\"\n",
        "\n",
        "    for i in tqdm(range(df.shape[0])):\n",
        "\n",
        "        # wrist symmetry : ratio of distances of left_wrist and right_wrist from sagittal plane\n",
        "        \"\"\"sagittal_plane = get_plane(df[\"nose\"][i], (df[\"left_shoulder\"][i]+df[\"right_shoulder\"][i])/2, (df[\"left_hip\"][i]+df[\"right_hip\"][i])/2)\n",
        "        denom = pow(pow(sagittal_plane[0],2) + pow(sagittal_plane[1],2) + pow(sagittal_plane[2],2), 0.5)\n",
        "\n",
        "        dist_wrist_left = abs(sagittal_plane[0]*df[\"left_wrist\"][i][0] + sagittal_plane[1]*df[\"left_wrist\"][i][1] + sagittal_plane[2]*df[\"left_wrist\"][i][2] + sagittal_plane[3]) / denom\n",
        "        dist_wrist_right = abs(sagittal_plane[0]*df[\"right_wrist\"][i][0] + sagittal_plane[1]*df[\"right_wrist\"][i][1] + sagittal_plane[2]*df[\"right_wrist\"][i][2] + sagittal_plane[3]) / denom\n",
        "        d[\"wrist_sagittal\"] = dist_wrist_left / dist_wrist_right\n",
        "\n",
        "        dist_elbow_left = abs(sagittal_plane[0]*df[\"left_elbow\"][i][0] + sagittal_plane[1]*df[\"left_elbow\"][i][1] + sagittal_plane[2]*df[\"left_elbow\"][i][2] + sagittal_plane[3]) / denom\n",
        "        dist_elbow_right = abs(sagittal_plane[0]*df[\"right_elbow\"][i][0] + sagittal_plane[1]*df[\"right_elbow\"][i][1] + sagittal_plane[2]*df[\"right_elbow\"][i][2] + sagittal_plane[3]) / denom\n",
        "        d[\"elbow_sagittal\"] = dist_elbow_left / dist_elbow_right\n",
        "\n",
        "        dist_knee_left = abs(sagittal_plane[0]*df[\"left_knee\"][i][0] + sagittal_plane[1]*df[\"left_knee\"][i][1] + sagittal_plane[2]*df[\"left_knee\"][i][2] + sagittal_plane[3]) / denom\n",
        "        dist_knee_right = abs(sagittal_plane[0]*df[\"right_knee\"][i][0] + sagittal_plane[1]*df[\"right_knee\"][i][1] + sagittal_plane[2]*df[\"right_knee\"][i][2] + sagittal_plane[3]) / denom\n",
        "        d[\"knee_sagittal\"] = dist_knee_left / dist_knee_right\n",
        "\n",
        "        dist_ankle_left = abs(sagittal_plane[0]*df[\"left_ankle\"][i][0] + sagittal_plane[1]*df[\"left_ankle\"][i][1] + sagittal_plane[2]*df[\"left_ankle\"][i][2] + sagittal_plane[3]) / denom\n",
        "        dist_ankle_right = abs(sagittal_plane[0]*df[\"right_ankle\"][i][0] + sagittal_plane[1]*df[\"right_ankle\"][i][1] + sagittal_plane[2]*df[\"right_ankle\"][i][2] + sagittal_plane[3]) / denom\n",
        "        d[\"ankle_sagittal\"] = dist_ankle_left / dist_ankle_right\"\"\"\n",
        "\n",
        "        ## hand angles\n",
        "        angle1 = get_angle(df[\"left_wrist\"][i], df[\"left_elbow\"][i], df[\"left_shoulder\"][i])\n",
        "        angle2 = get_angle(df[\"right_wrist\"][i], df[\"right_elbow\"][i], df[\"right_shoulder\"][i])\n",
        "\n",
        "        d[\"max_hand_angle\"].append(max(angle1,angle2))\n",
        "        d[\"min_hand_angle\"].append(min(angle1,angle2))\n",
        "\n",
        "        ## leg angles\n",
        "        angle1 = get_angle(df[\"left_ankle\"][i], df[\"left_knee\"][i], df[\"left_hip\"][i])\n",
        "        angle2 = get_angle(df[\"right_ankle\"][i], df[\"right_knee\"][i], df[\"right_hip\"][i])\n",
        "\n",
        "        d[\"max_foot_angle\"].append(max(angle1,angle2))\n",
        "        d[\"min_foot_angle\"].append(min(angle1,angle2))\n",
        "\n",
        "        ## hand to leg angles\n",
        "        # wrist to leg angle instead of elbow to knee\n",
        "        angle1 = get_angle(df[\"left_elbow\"][i], df[\"left_hip\"][i], df[\"left_knee\"][i])\n",
        "        angle2 = get_angle(df[\"right_elbow\"][i], df[\"right_hip\"][i], df[\"right_knee\"][i])\n",
        "        #angle1 = get_angle(df[\"left_wrist\"][i], df[\"left_hip\"][i], df[\"left_ankle\"][i])\n",
        "        #angle2 = get_angle(df[\"right_wrist\"][i], df[\"right_hip\"][i], df[\"right_ankle\"][i])\n",
        "\n",
        "        d[\"max_elbow_to_knee_angle\"].append(max(angle1,angle2))\n",
        "        d[\"min_elbow_to_knee_angle\"].append(min(angle1,angle2))\n",
        "\n",
        "        ## elbow to elbow\n",
        "        # try elbow-centroid-elbow instead\n",
        "        mid_point = (df[\"left_shoulder\"][i] + df[\"right_shoulder\"][i]) / 2\n",
        "        angle = get_angle(df[\"left_elbow\"][i], mid_point, df[\"right_elbow\"][i])\n",
        "\n",
        "        #centroid = np.mean(df.iloc[i])\n",
        "        #angle = get_angle(df[\"left_elbow\"][i], centroid, df[\"right_elbow\"][i])\n",
        "\n",
        "        d[\"elbow_to_elbow_angle\"].append(angle)\n",
        "\n",
        "        ## knee to knee\n",
        "        mid_point = (df[\"left_hip\"][i] + df[\"right_hip\"][i]) / 2\n",
        "        angle = get_angle(df[\"left_knee\"][i], mid_point, df[\"right_knee\"][i])\n",
        "\n",
        "        #angle = get_angle(df[\"left_knee\"][i], centroid, df[\"right_knee\"][i])\n",
        "        d[\"knee_to_knee_angle\"].append(angle)\n",
        "\n",
        "        ## heels at nose and hand at nose angle\n",
        "        # heel-waist-heel instead of heel-nose-heel\n",
        "        #angle1 = get_angle(df[\"left_wrist\"][i], df[\"nose\"][i], df[\"right_wrist\"][i])\n",
        "        #angle2 = get_angle(df[\"left_heel\"][i], df[\"nose\"][i], df[\"right_heel\"][i])\n",
        "        #mid_point = (df[\"left_hip\"][i] + df[\"right_hip\"][i]) / 2\n",
        "        #angle2 = get_angle(df[\"left_heel\"][i], mid_point, df[\"right_heel\"][i])\n",
        "\n",
        "        #d[\"hand_at_nose_angle\"].append(angle1)\n",
        "        #d[\"heel_at_nose_angle\"].append(angle2)\n",
        "\n",
        "        # nose-waist-heel\n",
        "        midpoint = (df[\"left_hip\"][i] + df[\"right_hip\"][i]) / 2\n",
        "        angle1 = get_angle(df[\"nose\"][i], midpoint, df[\"right_heel\"][i])\n",
        "        angle2 = get_angle(df[\"nose\"][i], midpoint, df[\"left_heel\"][i])\n",
        "\n",
        "        d[\"max_nose_to_heel_angle\"].append(max(angle1,angle2))\n",
        "\n",
        "        ############################################\n",
        "        \n",
        "        ## centroid distance \n",
        "        centroid = np.mean(df.iloc[i])\n",
        "        incl_cols = [\"left_wrist\", \"left_elbow\", \"left_shoulder\", \"left_eye\", \"left_hip\", \"left_knee\", \"left_ankle\",\n",
        "                     \"right_wrist\", \"right_elbow\", \"right_shoulder\", \"right_eye\", \"right_hip\", \"right_knee\", \"right_ankle\"]\n",
        "        for col in df.columns:\n",
        "  \n",
        "            if col in incl_cols:\n",
        "                \n",
        "                name = col+\"_centroid_distance\"\n",
        "                if name not in d:\n",
        "                    d[name] = []\n",
        "\n",
        "                #print(col, df[col][i])\n",
        "                d[name].append(np.sqrt(np.sum(np.square(df[col][i]-centroid))))\n",
        "        \n",
        "        ## feet and hand to shoulder ratio\n",
        "\n",
        "        d_shoulder = np.sqrt(np.sum(np.square(df[\"left_shoulder\"][i] - df[\"right_shoulder\"][i])))\n",
        "        d_hand = np.sqrt(np.sum(np.square(df[\"left_wrist\"][i] - df[\"right_wrist\"][i])))\n",
        "        d_feet = np.sqrt(np.sum(np.square(df[\"left_foot_index\"][i] - df[\"right_foot_index\"][i])))\n",
        "\n",
        "        d[\"feet_to_shoulder_ratio\"].append(d_feet/d_shoulder)\n",
        "        d[\"hand_to_shoulder_ratio\"].append(d_hand/d_shoulder)\n",
        "\n",
        "    ## output\n",
        "    if \"label\" in df:\n",
        "        d[\"label\"] = df[\"label\"]\n",
        "    \n",
        "    return pd.DataFrame(d)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PjDidNPm5pXP",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "def get_designed_data_results(df):\n",
        "    \n",
        "    ## converting output to numeric values\n",
        "    le.fit(df[\"Pose\"])\n",
        "    df[\"label\"] = le.transform(df[\"Pose\"])\n",
        "    \n",
        "    ## dropping Pose column\n",
        "    df.drop(columns=[\"Pose\",\"ImgNum\"],inplace=True)\n",
        "    \n",
        "   \n",
        "    ## feature designing for train and test\n",
        "    df = get_designed_data_df(df)\n",
        "    \n",
        "    x = df.iloc[:,:-1]\n",
        "    y = df.iloc[:,-1]\n",
        "    \n",
        "    x_train, x_test, y_train, y_test = train_test_split(x,y,stratify=y,test_size=0.25, random_state=0)\n",
        "    \n",
        "    \n",
        "    \n",
        "    model_dict = None\n",
        "    ## run models\n",
        "    model_dict = run_models(x_train, x_test, y_train, y_test,True)\n",
        "    \n",
        "    ## cosine similarity\n",
        "    #get_cosine_similarity_results(x_train, x_test, y_train, y_test, df.columns)\n",
        "    \n",
        "    return model_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ORkXX_i15pXP",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "def predict_image(img_path, model, le, design_features=True):\n",
        "    \n",
        "    ## get image keypoints\n",
        "    mp_pose = mp.solutions.pose\n",
        "    mp_drawing = mp.solutions.drawing_utils \n",
        "    mp_drawing_styles = mp.solutions.drawing_styles\n",
        "\n",
        "    \n",
        "    with mp_pose.Pose(static_image_mode=True, min_detection_confidence=0.5, model_complexity=2) as pose:\n",
        "        \n",
        "        landmark_names = []\n",
        "        \n",
        "        for name in mp_pose.PoseLandmark:\n",
        "            name = str(name).lower().split(\".\")[-1]\n",
        "            landmark_names.append(name)\n",
        "            \n",
        "        img = cv2.imread(img_path)\n",
        "        results = pose.process(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
        "      \n",
        "        row = {}\n",
        "        for i in range(0,33):\n",
        "            coord = results.pose_landmarks.landmark[i]\n",
        "  \n",
        "            if landmark_names[i] not in row:\n",
        "                row[landmark_names[i]] = []\n",
        "            \n",
        "            row[landmark_names[i]].append(np.array([coord.x, coord.y, coord.z]))\n",
        "        \n",
        "    df = pd.DataFrame(row)\n",
        "    \n",
        "    if design_features:\n",
        "        \n",
        "        df = get_custom_features(df)\n",
        "        \n",
        "    \n",
        "    #val = model.predict_proba(df)[0][model.predict(df)]\n",
        "    \n",
        "    #if val < 5/82:\n",
        "     #   print(\"PREDICTION UNCERTAIN\")\n",
        "        \n",
        "    name = le.inverse_transform(model.predict(df))\n",
        "    print(\"predicted_pose:\",name)\n",
        "    print(\"getting cosine similarity results...\")\n",
        "    y_test = [\"\" for i in range(df.shape[0])]\n",
        "    y_pred = model.predict(df)\n",
        "\n",
        "    cosine_eval(df, [1 for i in range(df.shape[0])], y_pred, df.columns)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Preprocess input"
      ],
      "metadata": {
        "id": "fBCgEw1gPNz9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6zhUbXFp5xX3"
      },
      "outputs": [],
      "source": [
        "#1. one-hot encode pose\n",
        "le = preprocessing.LabelEncoder()\n",
        "df = pd.read_csv(\"trainSet_yoga82.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Results"
      ],
      "metadata": {
        "id": "wOVrKsXvVdbM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.1 Keypoints as features [baseline]"
      ],
      "metadata": {
        "id": "3On9j-QRVfBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_dict = get_raw_data_results(df)"
      ],
      "metadata": {
        "id": "bmswdMq3Vcfa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f4eaa0d-54c0-4947-f0e4-ef0e21bc9c82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "##################################################\n",
            "LOGISTIC REGRESSION\n",
            "Testing Data:\n",
            "Accuracy: 0.6853162822562229\n",
            "F1 Score: 0.676068188761824\n",
            "--------------------------------------------------\n",
            "Training Data:\n",
            "Accuracy: 0.7102938937109792\n",
            "F1 Score: 0.7038607057916928\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.2 Geometric relations as features"
      ],
      "metadata": {
        "id": "kmCH7FaPVp2A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "d = get_designed_data_results(df)"
      ],
      "metadata": {
        "id": "sEl1bSE6Vuzn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ff23b87-aeab-4bb8-cd40-d7a20cbe5991"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 17513/17513 [00:55<00:00, 313.83it/s]\n",
            "100%|██████████| 17513/17513 [00:23<00:00, 757.85it/s]\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "##################################################\n",
            "LOGISTIC REGRESSION\n",
            "Testing Data:\n",
            "Accuracy: 0.5172413793103449\n",
            "F1 Score: 0.4951959748881769\n",
            "--------------------------------------------------\n",
            "Training Data:\n",
            "Accuracy: 0.5266483934825643\n",
            "F1 Score: 0.5083430772877348\n",
            "##################################################\n",
            "SGD CLASSIFIER\n",
            "Testing Data:\n",
            "Accuracy: 0.15688513359214432\n",
            "F1 Score: 0.14463000582740593\n",
            "--------------------------------------------------\n",
            "Training Data:\n",
            "Accuracy: 0.15692096847875742\n",
            "F1 Score: 0.14548498306385146\n",
            "##################################################\n",
            "GAUSSIAN NAIVE BAYES\n",
            "Testing Data:\n",
            "Accuracy: 0.8833066910253482\n",
            "F1 Score: 0.884005508979424\n",
            "--------------------------------------------------\n",
            "Training Data:\n",
            "Accuracy: 0.8934064260697426\n",
            "F1 Score: 0.8939559353149742\n",
            "##################################################\n",
            "RANDOM FOREST\n",
            "Testing Data:\n",
            "Accuracy: 0.9278374058004111\n",
            "F1 Score: 0.9274424741762507\n",
            "--------------------------------------------------\n",
            "Training Data:\n",
            "Accuracy: 1.0\n",
            "F1 Score: 1.0\n",
            "##################################################\n",
            "XGBOOST\n",
            "Testing Data:\n",
            "Accuracy: 0.9125371089289792\n",
            "F1 Score: 0.9122518950732736\n",
            "--------------------------------------------------\n",
            "Training Data:\n",
            "Accuracy: 0.9969544693162784\n",
            "F1 Score: 0.996954078507356\n",
            "##################################################\n",
            "ADABOOST\n",
            "Testing Data:\n",
            "Accuracy: 0.0701073304407399\n",
            "F1 Score: 0.009492101807305623\n",
            "--------------------------------------------------\n",
            "Training Data:\n",
            "Accuracy: 0.0702756205268768\n",
            "F1 Score: 0.01003501538883573\n",
            "##################################################\n",
            "KNN CLASSIFIER\n",
            "Testing Data:\n",
            "Accuracy: 0.5944279515871204\n",
            "F1 Score: 0.5845152815715018\n",
            "--------------------------------------------------\n",
            "Training Data:\n",
            "Accuracy: 0.6946855489569057\n",
            "F1 Score: 0.6893246534740088\n",
            "##################################################\n",
            "SVM\n",
            "Testing Data:\n",
            "Accuracy: 0.5361954784197306\n",
            "F1 Score: 0.5074250320181612\n",
            "--------------------------------------------------\n",
            "Training Data:\n",
            "Accuracy: 0.5422567382366378\n",
            "F1 Score: 0.5140814513758741\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Testing on random images"
      ],
      "metadata": {
        "id": "Zl0CWME7Vwtk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JnNQAYX95pXQ"
      },
      "outputs": [],
      "source": [
        "path =  \"/home/hardeekh/Downloads/custom_dataset/testing/\"#.jpg\"\n",
        "with open(\"custom_feature_model_dict.pkl\",\"rb\") as handle:\n",
        "    model_dict = pickle.load(handle)\n",
        "    \n",
        "for i in range(1,20):\n",
        "    \n",
        "    lol = path+str(i)+\".jpg\"\n",
        "    print(i)\n",
        "    y_pred=predict_image(lol, model_dict[\"RF\"], le, True)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "demo_final.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
